{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3bd88643-782c-4688-a995-1ae740f86f29",
   "metadata": {},
   "source": [
    "# Regularization in Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe6c23a-cc76-425a-b35e-8ac3b4a1428c",
   "metadata": {},
   "source": [
    "**Regularization is a technique used in machine learning to prevent overfitting, which otherwise causes models to perform poorly on unseen data. By adding a penalty for complexity, regularization encourages simpler and more generalizable models.**\n",
    "\n",
    "- Prevents overfitting: Adds constraints to the model to reduce the risk of memorizing noise in the training data.\n",
    "- Improves generalization: Encourages simpler models that perform better on new, unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a59fb01-d1ca-430e-b51e-261254544119",
   "metadata": {},
   "source": [
    "## Types of Regularization\n",
    "- There are mainly 3 types of regularization techniques, each applying penalties in different ways to control model complexity and improve generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc38992-aca6-4b62-9447-5b16a3b74c2a",
   "metadata": {},
   "source": [
    "## 1. Lasso Regression\n",
    "- A regression model which uses the L1 Regularization technique is called LASSO (Least Absolute Shrinkage and Selection Operator) regression. It adds the absolute value of magnitude of the coefficient as a penalty term to the loss function(L). This penalty can shrink some coefficients to zero which helps in selecting only the important features and ignoring the less important ones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989eca2e-e4c9-40b1-93d7-0e09ab0eb321",
   "metadata": {},
   "source": [
    "## Lets see how to implement this using python:\n",
    "\n",
    "- X, y = make_regression(n_samples=100, n_features=5, noise=0.1, random_state=42): Generates a regression dataset with 100 samples, 5 features and some noise.\n",
    "\n",
    "- X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42): Splits the data into 80% training and 20% testing sets.\n",
    "lasso = Lasso(alpha=0.1): Creates a Lasso regression model with regularization strength alpha set to 0.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1050a0d-6bb9-4837-93f7-874fb6c795b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.06362439921332522\n",
      "Coefficients: [60.50305581 98.52475354 64.3929265  56.96061238 35.52928502]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "X, y = make_regression(n_samples=100, n_features=5, noise=0.1, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "lasso = Lasso(alpha=0.1)\n",
    "lasso.fit(X_train, y_train)\n",
    "\n",
    "y_pred = lasso.predict(X_test)\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "\n",
    "print(\"Coefficients:\", lasso.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd6c601-3c3d-4154-9bce-7b59e49a7415",
   "metadata": {},
   "source": [
    "## 2. Ridge Regression\n",
    "- A regression model that uses the L2 regularization technique is called Ridge regression. It adds the squared magnitude of the coefficient as a penalty term to the loss function(L). It handles multicollinearity by shrinking the coefficients of correlated features instead of eliminating them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5659f908-626e-4a4b-8f3d-b13918bdbcb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 4.114050771972589\n",
      "Coefficients: [59.87954432 97.15091098 63.24364738 56.31999433 35.34591136]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "X, y = make_regression(n_samples=100, n_features=5, noise=0.1, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "ridge = Ridge(alpha=1.0)\n",
    "ridge.fit(X_train, y_train)\n",
    "y_pred = ridge.predict(X_test)\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"Coefficients:\", ridge.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18637917-d153-444d-bbbf-2f50b54f681a",
   "metadata": {},
   "source": [
    "## 3. Elastic Net Regression\n",
    "- Elastic Net Regression is a combination of both L1 as well as L2 regularization. That shows that we add the absolute norm of the weights as well as the squared measure of the weights. With the help of an extra hyperparameter that controls the ratio of the L1 and L2 regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4052f0c0-1fbd-493e-9574-162ed79fd339",
   "metadata": {},
   "source": [
    "Lets see how to implement this using python:\n",
    "\n",
    "- model = ElasticNet(alpha=1.0, l1_ratio=0.5) : Creates an Elastic Net model with regularization strength alpha=1.0 and L1/L2 mixing ratio 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56f006ed-9f67-4b1c-8068-849bdafe72df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 7785.886176938016\n",
      "Coefficients: [16.84528938 31.77080959  4.05901996 40.18486737 57.25856154 45.81463318\n",
      " 58.97979422 -0.          3.82816854 41.1096051 ]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "X, y = make_regression(n_samples=100, n_features=10, noise=0.1, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = ElasticNet(alpha=1.0, l1_ratio=0.5)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"Coefficients:\", model.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8484d76-bd29-4511-a020-15f74f6585b6",
   "metadata": {},
   "source": [
    "## Benefits of Regularization\n",
    "\n",
    "- Prevents Overfitting: Regularization helps models focus on underlying patterns instead of memorizing noise in the training data.\n",
    "- Enhances Performance: Prevents excessive weighting of outliers or irrelevant features helps in improving overall model accuracy.\n",
    "- Stabilizes Models: Reduces sensitivity to minor data changes which ensures consistency across different data subsets.\n",
    "- Prevents Complexity: Keeps model from becoming too complex which is important for limited or noisy data.\n",
    "- Handles Multicollinearity: Reduces the magnitudes of correlated coefficients helps in improving model stability.\n",
    "- Promotes Consistency: Ensures reliable performance across different datasets which reduces the risk of large performance shifts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b3feb9-0395-4c7e-a1ca-c4e24f4d3f91",
   "metadata": {},
   "source": [
    "# The End !!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
