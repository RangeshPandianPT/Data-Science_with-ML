{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95eac977-982b-431e-a664-ba662799ec27",
   "metadata": {},
   "source": [
    "# Random Forest\n",
    "\n",
    "- Random forest is an ensemble learning method that combines many decision trees to make a final prediction.\n",
    "\n",
    "- Each decision tree is trained on a random subset of data and features, which increases diversity and reduces overfitting.\n",
    "\n",
    "- For classification, the final output is chosen by majority voting among all the trees.\n",
    "\n",
    "- For regression, the final prediction is the average of all tree predictions.\n",
    "\n",
    "- Random forest is more accurate and stable than a single decision tree because it reduces variance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0157b70f-9875-4839-807a-63e9d97cb41c",
   "metadata": {},
   "source": [
    "## 1. Important Terms in Random Forest \n",
    "\n",
    "1. **Root Node**\n",
    "- The starting point of the tree where the entire training data is given. The first split happens at this node.\n",
    "\n",
    "2. **Splitting**\n",
    "- The process of dividing data into smaller groups using conditions. Methods like Gini and Entropy are used to select the best split.\n",
    "\n",
    "3. **Decision Nodes**\n",
    "- Nodes that appear after splitting and lead the path toward leaf nodes. They contain further conditions based on features.\n",
    "\n",
    "4. **Leaf Nodes (Terminal Nodes)**\n",
    "- These are the endpoints of the tree where no further splitting occurs. Final predictions are made here.\n",
    "\n",
    "5. **Random Forest Context**\n",
    "- In a random forest, many such trees are created and each tree uses these nodes. The final prediction is based on the combined decision of multiple trees."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10e36b5-0c4d-4279-acd0-436ba183c8e8",
   "metadata": {},
   "source": [
    "## 2. Working of Random Forest\n",
    "\n",
    "1. **Random Sampling with Replacement**\n",
    "- Each tree in the random forest is trained on a bootstrap sample, meaning a random subset of the dataset is selected with replacement.\n",
    "\n",
    "2. **Feature Selection for Splits**\n",
    "- At each split in a tree, only a random subset of features is considered. This reduces correlation between trees and improves accuracy.\n",
    "\n",
    "3. **Best Split Selection**\n",
    "- Each decision tree chooses the best split using measures like Gini Impurity or Information Gain to separate classes effectively.\n",
    "\n",
    "4. **Bootstrap Aggregation (Bagging)**\n",
    "- Random forest is an ensemble technique that uses bagging. Each tree makes a prediction and the final output is based on majority vote or averaging.\n",
    "\n",
    "5. **Improved Stability and Accuracy**\n",
    "- Combining multiple trees reduces variance and makes the model more stable compared to a single decision tree."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1304e59e-e94d-4fa7-bf89-aab8b46324c7",
   "metadata": {},
   "source": [
    "### 2.1 Feature Selection in Random Forest:\n",
    "\n",
    "**Classification Problems**\n",
    "- Random forest selects features by default using the square root of the total number of features.\n",
    "For example, if there are 16 features, only sqrt(16) which is 4 features are considered at each split.\n",
    "\n",
    "**Regression Problems**\n",
    "- Random forest selects features by default using one third of the total number of features.\n",
    "For example, if there are 15 features, only 15 divided by 3 which is 5 features are considered at each split.\n",
    "\n",
    "**Purpose of Random Feature Selection**\n",
    "- This randomness reduces correlations between trees and makes the ensemble more robust and accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d503afaa-4488-41ff-8990-a78958abd838",
   "metadata": {},
   "source": [
    "### 2.2 Bootstrap Aggregation (Bagging):\n",
    "\n",
    "1. **Multiple Decision Trees**\n",
    "- Random forest builds many decision trees. Each tree is trained on a different bootstrap sample, which means a random sample of the dataset taken with replacement.\n",
    "\n",
    "2. **Different Splits and Paths**\n",
    "- Because each tree gets different data, the splits chosen by the trees are different. This creates diversity in the model.\n",
    "\n",
    "3. **Prediction from Each Tree**\n",
    "- For a new input, every tree in the forest predicts a class label.\n",
    "Example shown: one tree predicts Chinstrap, another predicts Adelie, and others may also vote differently.\n",
    "\n",
    "4. **Majority Voting**\n",
    "- The final prediction is based on majority vote among all the trees.\n",
    "Example: If 2 trees predict Chinstrap and 1 predicts Adelie, the output becomes Chinstrap.\n",
    "\n",
    "5. **Why Bagging Works**\n",
    "- It reduces variance and prevents overfitting by averaging the predictions of multiple diverse trees.\n",
    "The ensemble becomes more accurate and stable than any single decision tree."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc20232c-18f1-4dc9-b874-1f86d6189880",
   "metadata": {},
   "source": [
    "### 2.3 Splitting Methods\n",
    "\n",
    "**Gini Impurity**\n",
    "\n",
    "- Measures how often a randomly chosen sample would be misclassified.\n",
    "\n",
    "- Value ranges from 0 to 1, where 0 means perfectly pure and 1 means highly impure.\n",
    "\n",
    "- Lower Gini is preferred for splits.\n",
    "\n",
    "**Information Gain**\n",
    "\n",
    "- Selects the feature that gives the most information about the class.\n",
    "\n",
    "- Calculated using entropy before and after the split.\n",
    "\n",
    "- Higher information gain means a better split.\n",
    "\n",
    "**Entropy**\n",
    "\n",
    "- Measures randomness or uncertainty in the data.\n",
    "\n",
    "- High entropy means mixed classes; low entropy means purer nodes.\n",
    "\n",
    "- Used in calculating information gain.\n",
    "\n",
    "**How Splitting Works**\n",
    "\n",
    "- At each node, the algorithm evaluates all features using Gini or Information Gain.\n",
    "\n",
    "- The feature and threshold giving the best purity improvement is selected for the split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b60e93-4536-4c6d-93d1-dfb85d483a06",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
